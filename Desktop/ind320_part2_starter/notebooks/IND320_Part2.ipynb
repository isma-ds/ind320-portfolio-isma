{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55f99ab",
   "metadata": {},
   "source": [
    "# IND320 — Part 2: Elhub → Spark/Cassandra → MongoDB → Streamlit\n",
    "**Student:** Isma Sohail  \n",
    "**Course:** IND320 Data to Decision  \n",
    "**Part:** 2\n",
    "\n",
    "This Notebook implements the required pipeline:\n",
    "\n",
    "1. Fetch hourly production data for 2021 from Elhub API (`PRODUCTION_PER_GROUP_MBA_HOUR`).\n",
    "2. Normalize JSON → `pandas.DataFrame` (only the `productionPerGroupMbaHour` list).\n",
    "3. Insert data into Cassandra using Spark.\n",
    "4. Use Spark to read and select: `priceArea, productionGroup, startTime, quantityKwh`.\n",
    "5. Create plots: (i) **Pie** = total year by production group for a chosen price area; (ii) **Line** = first month by production group for a chosen price area.\n",
    "6. Insert the Spark-extracted data into **MongoDB**.\n",
    "7. Fill the **AI usage** note and a **300–500 word log**.\n",
    "\n",
    "> **Tip:** If Docker is unavailable on your laptop, use **managed services**:\n",
    "- **Cassandra** → DataStax **Astra DB** (free-tier, compatible with Spark connector).\n",
    "- **MongoDB** → **MongoDB Atlas** free-tier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb1a36",
   "metadata": {},
   "source": [
    "## 0. Setup (env, packages, secrets)\n",
    "Fill in `.streamlit/secrets.toml` or export environment variables locally. The Streamlit app will read the same secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae660b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil import tz\n",
    "import requests\n",
    "\n",
    "ELHUB_API_BASE = os.environ.get(\"ELHUB_API_BASE\", \"https://api.elhub.no/energy-data-api\")\n",
    "ELHUB_API_TOKEN = os.environ.get(\"ELHUB_API_TOKEN\")  # Bearer token from Elhub portal\n",
    "assert ELHUB_API_TOKEN is not None, \"Set ELHUB_API_TOKEN (bearer token) via env vars or secrets.\"\n",
    "\n",
    "MONGODB_URI = os.environ.get(\"MONGODB_URI\", \"\")\n",
    "MONGODB_DB = os.environ.get(\"MONGODB_DB\", \"ind320\")\n",
    "MONGODB_COLLECTION = os.environ.get(\"MONGODB_COLLECTION\", \"elhub_production\")\n",
    "\n",
    "print(\"Base:\", ELHUB_API_BASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d341e44",
   "metadata": {},
   "source": [
    "## 1. Elhub API fetch — `PRODUCTION_PER_GROUP_MBA_HOUR` (year 2021)\n",
    "**Observe:**\n",
    "- Time is encoded in ISO-8601 (UTC). Handle CET/CEST transitions for Europe/Oslo if converting.\n",
    "- Requests have **period limits** → fetch **month-by-month**.\n",
    "- Keep only `productionPerGroupMbaHour` list.\n",
    "**Fields:** `priceArea`, `productionGroup`, `startTime`, `quantityKwh`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b648da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def elhub_headers():\n",
    "    return {\"Authorization\": f\"Bearer {ELHUB_API_TOKEN}\", \"Accept\": \"application/json\"}\n",
    "\n",
    "def fetch_month(year: int, month: int):\n",
    "    endpoint = f\"{ELHUB_API_BASE}/v1/production/PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "    start = datetime(year, month, 1, tzinfo=timezone.utc)\n",
    "    end = start + relativedelta(months=1)\n",
    "    params = {\"startTime\": start.isoformat(), \"endTime\": end.isoformat(), \"aggregation\": \"Hour\"}\n",
    "    r = requests.get(endpoint, headers=elhub_headers(), params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"productionPerGroupMbaHour\", [])\n",
    "\n",
    "all_rows = []\n",
    "for m in range(1, 13):\n",
    "    print(f\"Fetching 2021-{m:02d} ...\")\n",
    "    all_rows.extend(fetch_month(2021, m))\n",
    "len(all_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw_df = pd.json_normalize(all_rows)\n",
    "colmap = {\"priceArea\":\"priceArea\",\"productionGroup\":\"productionGroup\",\"startTime\":\"startTime\",\"quantityKwh\":\"quantityKwh\"}\n",
    "df = raw_df[list(colmap.keys())].rename(columns=colmap).copy()\n",
    "df[\"startTime\"] = pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c54fbf",
   "metadata": {},
   "source": [
    "### (Fallback) Create a tiny sample if API access is blocked\n",
    "Run **only** if the cell above failed. This unblocks later steps during development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.empty:\n",
    "    import numpy as np\n",
    "    rng = pd.date_range(\"2021-01-01\", \"2021-01-31 23:00\", freq=\"H\", tz=\"UTC\")\n",
    "    demo = []\n",
    "    for area in [\"NO1\",\"NO2\"]:\n",
    "        for grp in [\"WIND\",\"HYDRO\",\"THERMAL\"]:\n",
    "            demo.extend([{\"priceArea\": area, \"productionGroup\": grp, \"startTime\": t, \"quantityKwh\": int(np.random.randint(1000, 5000))} for t in rng])\n",
    "    df = pd.DataFrame(demo)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c1b58",
   "metadata": {},
   "source": [
    "## 2. Insert to Cassandra using Spark (Astra)\n",
    "Download secure connect bundle and set env vars: `ASTRA_SECURE_BUNDLE_PATH`, `ASTRA_CLIENT_ID`, `ASTRA_CLIENT_SECRET`.\n",
    "Create keyspace `ind320` first in Astra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7391cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "astra_bundle = os.environ.get(\"ASTRA_SECURE_BUNDLE_PATH\")\n",
    "astra_client_id = os.environ.get(\"ASTRA_CLIENT_ID\")\n",
    "astra_client_secret = os.environ.get(\"ASTRA_CLIENT_SECRET\")\n",
    "assert astra_bundle and astra_client_id and astra_client_secret, \"Set Astra bundle + client id/secret env vars.\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IND320-Elhub-Cassandra\") \\\n",
    "    .config(\"spark.cassandra.connection.config.cloud.path\", astra_bundle) \\\n",
    "    .config(\"spark.cassandra.auth.username\", astra_client_id) \\\n",
    "    .config(\"spark.cassandra.auth.password\", astra_client_secret) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sdf = spark.createDataFrame(df)\n",
    "keyspace = \"ind320\"\n",
    "table = \"elhub_production_2021\"\n",
    "sdf.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\").options(keyspace=keyspace, table=table).save()\n",
    "print(\"Written to Cassandra:\", f\"{keyspace}.{table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d080d",
   "metadata": {},
   "source": [
    "## 3. Read back & select required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = spark.read.format(\"org.apache.spark.sql.cassandra\").options(keyspace=keyspace, table=table).load()\n",
    "sel = sdf2.select(\"priceArea\",\"productionGroup\",\"startTime\",\"quantityKwh\")\n",
    "sel.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b13749",
   "metadata": {},
   "source": [
    "## 4. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = sel.toPandas()\n",
    "pdf[\"startTime\"] = pd.to_datetime(pdf[\"startTime\"], utc=True)\n",
    "areas = sorted(pdf[\"priceArea\"].unique().tolist())\n",
    "chosen = areas[0] if areas else \"NO1\"\n",
    "pie_df = pdf[pdf[\"priceArea\"] == chosen].groupby(\"productionGroup\", as_index=False)[\"quantityKwh\"].sum()\n",
    "pie_df.plot(kind=\"pie\", y=\"quantityKwh\", labels=pie_df[\"productionGroup\"], autopct=\"%1.1f%%\", legend=False, ylabel=\"\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(f\"Total production 2021 — {chosen}\")\n",
    "plt.show()\n",
    "\n",
    "# Line — month 1\n",
    "jan = pdf[(pdf[\"priceArea\"] == chosen) & (pdf[\"startTime\"].dt.month == 1)].copy()\n",
    "for grp, sub in jan.groupby(\"productionGroup\"):\n",
    "    sub = sub.sort_values(\"startTime\")\n",
    "    plt.plot(sub[\"startTime\"], sub[\"quantityKwh\"], label=grp)\n",
    "plt.legend(); plt.title(f\"Hourly production — January 2021 — {chosen}\"); plt.xlabel(\"Time\"); plt.ylabel(\"kWh\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2956a10",
   "metadata": {},
   "source": [
    "## 5. Insert selected data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(MONGODB_URI) if MONGODB_URI else None\n",
    "if client is None:\n",
    "    raise RuntimeError(\"Set MONGODB_URI to insert into MongoDB Atlas.\")\n",
    "mdb = client[MONGODB_DB]\n",
    "coll = mdb[MONGODB_COLLECTION]\n",
    "coll.delete_many({})\n",
    "records = pdf[[\"priceArea\",\"productionGroup\",\"startTime\",\"quantityKwh\"]].copy()\n",
    "records[\"startTime\"] = records[\"startTime\"].dt.tz_convert(\"UTC\").dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "coll.insert_many(records.to_dict(orient=\"records\"))\n",
    "coll.count_documents({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e35b92",
   "metadata": {},
   "source": [
    "## 6. AI usage (brief)\n",
    "Write 2–4 sentences about AI assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86f9f5",
   "metadata": {},
   "source": [
    "_(Fill here in your own words.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ac6ad",
   "metadata": {},
   "source": [
    "## 7. Work log (300–500 words)\n",
    "Describe Notebook + Streamlit experience, auth, DST, limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f07f3d",
   "metadata": {},
   "source": [
    "_(Write 300–500 words here.)_"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
